{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82479eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>year</th>\n",
       "      <th>country</th>\n",
       "      <th>country_name</th>\n",
       "      <th>speaker</th>\n",
       "      <th>position</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>ALB</td>\n",
       "      <td>Albania</td>\n",
       "      <td>Mr. NAS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33: May I first convey to our President the co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>ARG</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Mr. DE PABLO PARDO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>177.\\t : It is a fortunate coincidence that pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>AUS</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Mr. McMAHON</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.\\t  It is a pleasure for me to extend to y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>AUT</td>\n",
       "      <td>Austria</td>\n",
       "      <td>Mr. KIRCHSCHLAEGER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>155.\\t  May I begin by expressing to Ambassado...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>BEL</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>Mr. HARMEL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>176. No doubt each of us, before coming up to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>BLR</td>\n",
       "      <td>Belarus</td>\n",
       "      <td>Mr. GURINOVICH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n71.\\t. We are today mourning the untimely de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>BOL</td>\n",
       "      <td>Bolivia, Plurinational State of</td>\n",
       "      <td>Mr. CAMACHO OMISTE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>135.\\t  I wish to congratulate the President o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>BRA</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Mr. GIBSON BARBOZA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.\\tMr. President, I should like, first of all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>CAN</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Mr. SHARP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nThe General Assembly is fortunate indeed to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>CMR</td>\n",
       "      <td>Cameroon</td>\n",
       "      <td>Mr. AHIDJO</td>\n",
       "      <td>President</td>\n",
       "      <td>: A year ago I came here as the Acting Preside...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>COG</td>\n",
       "      <td>Congo</td>\n",
       "      <td>Mr. ICKONGA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122.\\t  I cannot begin my intervention without...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>COL</td>\n",
       "      <td>Colombia</td>\n",
       "      <td>Mr. VASQUEZ CARRIZOSA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mr. President, this visit to the United Nation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>CRI</td>\n",
       "      <td>Costa Rica</td>\n",
       "      <td>Mr. FACIO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.\\t  Mr. President, your election to the Pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>CUB</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>Mr. ALARCON</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.\\t  Mr. President, I should first like to co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>DOM</td>\n",
       "      <td>Dominican Republic</td>\n",
       "      <td>Mr FERNANDEZ G.</td>\n",
       "      <td></td>\n",
       "      <td>\\n\\n\\n Mr. President, it was a source of great...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>DZA</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>Mr. YAZID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.  The delegation of Algeria is very pleased ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>ECU</td>\n",
       "      <td>Ecuador</td>\n",
       "      <td>Mr. Benites</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71.  It had been my hope that a loftier person...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>FRA</td>\n",
       "      <td>France</td>\n",
       "      <td>Mr. SCHUMANN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.\\t  Within one month, when we celebrate the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>GBR</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Sir Alec DOUGLASHOME</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110.\\t Mr. President, I should like first to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>GHA</td>\n",
       "      <td>Ghana</td>\n",
       "      <td>Mr. OWUSU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>121.\\t I should like to begin by congratulatin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    session  year country                     country_name  \\\n",
       "0        25  1970     ALB                          Albania   \n",
       "1        25  1970     ARG                        Argentina   \n",
       "2        25  1970     AUS                        Australia   \n",
       "3        25  1970     AUT                          Austria   \n",
       "4        25  1970     BEL                          Belgium   \n",
       "5        25  1970     BLR                          Belarus   \n",
       "6        25  1970     BOL  Bolivia, Plurinational State of   \n",
       "7        25  1970     BRA                           Brazil   \n",
       "8        25  1970     CAN                           Canada   \n",
       "9        25  1970     CMR                         Cameroon   \n",
       "10       25  1970     COG                            Congo   \n",
       "11       25  1970     COL                         Colombia   \n",
       "12       25  1970     CRI                       Costa Rica   \n",
       "13       25  1970     CUB                             Cuba   \n",
       "14       25  1970     DOM               Dominican Republic   \n",
       "15       25  1970     DZA                          Algeria   \n",
       "16       25  1970     ECU                          Ecuador   \n",
       "17       25  1970     FRA                           France   \n",
       "18       25  1970     GBR                   United Kingdom   \n",
       "19       25  1970     GHA                            Ghana   \n",
       "\n",
       "                  speaker    position  \\\n",
       "0                 Mr. NAS         NaN   \n",
       "1      Mr. DE PABLO PARDO         NaN   \n",
       "2             Mr. McMAHON         NaN   \n",
       "3      Mr. KIRCHSCHLAEGER         NaN   \n",
       "4              Mr. HARMEL         NaN   \n",
       "5          Mr. GURINOVICH         NaN   \n",
       "6      Mr. CAMACHO OMISTE         NaN   \n",
       "7      Mr. GIBSON BARBOZA         NaN   \n",
       "8               Mr. SHARP         NaN   \n",
       "9              Mr. AHIDJO  President    \n",
       "10            Mr. ICKONGA         NaN   \n",
       "11  Mr. VASQUEZ CARRIZOSA         NaN   \n",
       "12              Mr. FACIO         NaN   \n",
       "13            Mr. ALARCON         NaN   \n",
       "14        Mr FERNANDEZ G.               \n",
       "15              Mr. YAZID         NaN   \n",
       "16            Mr. Benites         NaN   \n",
       "17           Mr. SCHUMANN         NaN   \n",
       "18   Sir Alec DOUGLASHOME         NaN   \n",
       "19              Mr. OWUSU         NaN   \n",
       "\n",
       "                                                 text  \n",
       "0   33: May I first convey to our President the co...  \n",
       "1   177.\\t : It is a fortunate coincidence that pr...  \n",
       "2   100.\\t  It is a pleasure for me to extend to y...  \n",
       "3   155.\\t  May I begin by expressing to Ambassado...  \n",
       "4   176. No doubt each of us, before coming up to ...  \n",
       "5   \\n71.\\t. We are today mourning the untimely de...  \n",
       "6   135.\\t  I wish to congratulate the President o...  \n",
       "7   1.\\tMr. President, I should like, first of all...  \n",
       "8   \\nThe General Assembly is fortunate indeed to ...  \n",
       "9   : A year ago I came here as the Acting Preside...  \n",
       "10  122.\\t  I cannot begin my intervention without...  \n",
       "11  Mr. President, this visit to the United Nation...  \n",
       "12  62.\\t  Mr. President, your election to the Pre...  \n",
       "13  1.\\t  Mr. President, I should first like to co...  \n",
       "14  \\n\\n\\n Mr. President, it was a source of great...  \n",
       "15  1.  The delegation of Algeria is very pleased ...  \n",
       "16  71.  It had been my hope that a loftier person...  \n",
       "17  84.\\t  Within one month, when we celebrate the...  \n",
       "18  110.\\t Mr. President, I should like first to s...  \n",
       "19  121.\\t I should like to begin by congratulatin...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file = \"/Users/srijanagella/Documents/un-general-debates-blueprint.csv\"  \n",
    "df = pd.read_csv(file)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "443b7473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\ufeffIt is indeed a pleasure for me and the members of my delegation to extend to Ambassador Garba our sincere congratulations on his election to the presidency of the forty-fourth session of the General Assembly. His election to this high office is a well-deserved tribute to his personal qualities and experience. I am fully confident that under his able and wise leadership the Assembly will further c'\n",
      "'\\ufeffI wish to join\\nother representatives in congratulating you, Sir, on\\nyour unanimous election as President of the fifty-sixth\\nsession of the General Assembly. We are confident that\\n27\\n\\nunder your able guidance the work of this General\\nAssembly session will be another milestone on the new\\ninternational scene, particularly in confronting the new\\nchallenges facing our world, especially after the\\nextre'\n"
     ]
    }
   ],
   "source": [
    "print(repr(df.iloc[2666][\"text\"][0:400]))\n",
    "print(repr(df.iloc[4726][\"text\"][0:400]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d7fc3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "df[\"paragraphs\"] = df[\"text\"].map(lambda text: re.split('\\.\\s*\\n', text))\n",
    "df[\"number_of_paragraphs\"] = df[\"paragraphs\"].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "399d3320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/srijanagella/anaconda3/lib/python3.11/site-packages (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/srijanagella/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/srijanagella/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/srijanagella/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/srijanagella/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/srijanagella/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/srijanagella/anaconda3/lib/python3.11/site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/srijanagella/anaconda3/lib/python3.11/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/srijanagella/anaconda3/lib/python3.11/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/srijanagella/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/srijanagella/anaconda3/lib/python3.11/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/srijanagella/anaconda3/lib/python3.11/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/srijanagella/anaconda3/lib/python3.11/site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/srijanagella/anaconda3/lib/python3.11/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/srijanagella/anaconda3/lib/python3.11/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/srijanagella/anaconda3/lib/python3.11/site-packages (from spacy) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in /Users/srijanagella/anaconda3/lib/python3.11/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/srijanagella/anaconda3/lib/python3.11/site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/srijanagella/anaconda3/lib/python3.11/site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/srijanagella/anaconda3/lib/python3.11/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/srijanagella/anaconda3/lib/python3.11/site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/srijanagella/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/srijanagella/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/srijanagella/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/srijanagella/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/srijanagella/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/srijanagella/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/srijanagella/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/srijanagella/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/srijanagella/anaconda3/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/srijanagella/anaconda3/lib/python3.11/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a216d4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srijanagella/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7507, 24611)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "\n",
    "# Convert set of stop words to list\n",
    "stop_words_list = list(stopwords)\n",
    "\n",
    "tfidf_text_vectorizer = TfidfVectorizer(stop_words=stop_words_list, min_df=5, max_df=0.7)\n",
    "tfidf_text_vectors = tfidf_text_vectorizer.fit_transform(df['text'])\n",
    "tfidf_text_vectors.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efc36617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33: May I first convey to our President the co...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35.\\tThe utilization of the United Nations to ...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36.\\tThe whole of progressive mankind recalls ...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.\\tAll this has had well known consequences ...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38.\\tOne of the undeniable proofs that the Uni...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>39.\\tUndoubtedly, such a state of affairs in t...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40.\\tThe liberation movement at the world leve...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>41.\\tPanic-stricken at the impetuous growth of...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>42.\\tAlthough split by numerous contradictions...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>43.\\tIn that connexion we can cite, simultaneo...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  year\n",
       "0  33: May I first convey to our President the co...  1970\n",
       "1  35.\\tThe utilization of the United Nations to ...  1970\n",
       "2  36.\\tThe whole of progressive mankind recalls ...  1970\n",
       "3  37.\\tAll this has had well known consequences ...  1970\n",
       "4  38.\\tOne of the undeniable proofs that the Uni...  1970\n",
       "5  39.\\tUndoubtedly, such a state of affairs in t...  1970\n",
       "6  40.\\tThe liberation movement at the world leve...  1970\n",
       "7  41.\\tPanic-stricken at the impetuous growth of...  1970\n",
       "8  42.\\tAlthough split by numerous contradictions...  1970\n",
       "9  43.\\tIn that connexion we can cite, simultaneo...  1970"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatten the paragraphs keeping the years\n",
    "paragraph_df = pd.DataFrame([{ \"text\": paragraph, \"year\": year }\n",
    "                             for paragraphs, year in zip(df[\"paragraphs\"], df[\"year\"])\n",
    "                             for paragraph in paragraphs if paragraph])\n",
    "paragraph_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b59e3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srijanagella/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(279076, 25162)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "\n",
    "# Convert set of stop words to list\n",
    "stop_words_list = list(stopwords)\n",
    "\n",
    "tfidf_para_vectorizer = TfidfVectorizer(stop_words=stop_words_list, min_df=5, max_df=0.7)\n",
    "tfidf_para_vectors = tfidf_para_vectorizer.fit_transform(paragraph_df[\"text\"])\n",
    "tfidf_para_vectors.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad1df702",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srijanagella/anaconda3/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf_text_model = NMF(n_components=10, random_state=42)\n",
    "W_text_matrix = nmf_text_model.fit_transform(tfidf_text_vectors)\n",
    "H_text_matrix = nmf_text_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ad51793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 00\n",
      " co (0.79)\n",
      " operation (0.65)\n",
      " disarmament (0.36)\n",
      " nuclear (0.34)\n",
      " relations (0.25)\n",
      "\n",
      "Topic 01\n",
      " terrorism (0.38)\n",
      " challenges (0.32)\n",
      " sustainable (0.30)\n",
      " millennium (0.29)\n",
      " reform (0.28)\n",
      "\n",
      "Topic 02\n",
      " africa (1.15)\n",
      " african (0.82)\n",
      " south (0.63)\n",
      " namibia (0.36)\n",
      " delegation (0.30)\n",
      "\n",
      "Topic 03\n",
      " arab (1.02)\n",
      " israel (0.89)\n",
      " palestinian (0.60)\n",
      " lebanon (0.54)\n",
      " israeli (0.54)\n",
      "\n",
      "Topic 04\n",
      " american (0.33)\n",
      " america (0.31)\n",
      " latin (0.31)\n",
      " panama (0.21)\n",
      " bolivia (0.21)\n",
      "\n",
      "Topic 05\n",
      " pacific (1.55)\n",
      " islands (1.23)\n",
      " solomon (0.86)\n",
      " island (0.82)\n",
      " fiji (0.71)\n",
      "\n",
      "Topic 06\n",
      " soviet (0.81)\n",
      " republic (0.78)\n",
      " nuclear (0.68)\n",
      " viet (0.64)\n",
      " socialist (0.63)\n",
      "\n",
      "Topic 07\n",
      " guinea (4.26)\n",
      " equatorial (1.75)\n",
      " bissau (1.53)\n",
      " papua (1.47)\n",
      " republic (0.57)\n",
      "\n",
      "Topic 08\n",
      " european (0.61)\n",
      " europe (0.44)\n",
      " cooperation (0.39)\n",
      " bosnia (0.34)\n",
      " herzegovina (0.30)\n",
      "\n",
      "Topic 09\n",
      " caribbean (0.98)\n",
      " small (0.66)\n",
      " bahamas (0.63)\n",
      " saint (0.63)\n",
      " barbados (0.61)\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, vectorizer, no_top_words=5):\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    for topic, words in enumerate(model.components_):\n",
    "        total = words.sum()\n",
    "        largest = words.argsort()[::-1] # invert sort order\n",
    "        print(\"\\nTopic %02d\" % topic)\n",
    "        for i in range(0, no_top_words):\n",
    "            print(\" %s (%2.2f)\" % (feature_names[largest[i]],\n",
    "                                   abs(words[largest[i]]*100.0/total)))\n",
    "\n",
    "display_topics(nmf_text_model, tfidf_text_vectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d45e4a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 00\n",
      " nations (5.61)\n",
      " united (5.50)\n",
      " organization (1.27)\n",
      " states (1.02)\n",
      " charter (0.93)\n",
      "\n",
      "Topic 01\n",
      " general (2.86)\n",
      " session (2.83)\n",
      " assembly (2.81)\n",
      " mr (1.99)\n",
      " president (1.81)\n",
      "\n",
      "Topic 02\n",
      " countries (4.41)\n",
      " developing (2.49)\n",
      " economic (1.50)\n",
      " developed (1.35)\n",
      " trade (0.92)\n",
      "\n",
      "Topic 03\n",
      " people (1.36)\n",
      " peace (1.33)\n",
      " east (1.29)\n",
      " middle (1.17)\n",
      " palestinian (1.16)\n",
      "\n",
      "Topic 04\n",
      " nuclear (4.93)\n",
      " weapons (3.26)\n",
      " disarmament (2.01)\n",
      " treaty (1.71)\n",
      " proliferation (1.46)\n",
      "\n",
      "Topic 05\n",
      " rights (6.48)\n",
      " human (6.16)\n",
      " respect (1.15)\n",
      " fundamental (0.85)\n",
      " universal (0.82)\n",
      "\n",
      "Topic 06\n",
      " africa (3.80)\n",
      " south (3.30)\n",
      " african (1.70)\n",
      " namibia (1.38)\n",
      " apartheid (1.18)\n",
      "\n",
      "Topic 07\n",
      " security (6.10)\n",
      " council (5.87)\n",
      " permanent (1.49)\n",
      " reform (1.49)\n",
      " peace (1.29)\n",
      "\n",
      "Topic 08\n",
      " international (2.03)\n",
      " world (1.48)\n",
      " community (0.91)\n",
      " new (0.76)\n",
      " peace (0.67)\n",
      "\n",
      "Topic 09\n",
      " development (4.47)\n",
      " sustainable (1.19)\n",
      " economic (1.06)\n",
      " social (0.99)\n",
      " goals (0.94)\n"
     ]
    }
   ],
   "source": [
    "nmf_para_model = NMF(n_components=10, random_state=42)\n",
    "W_para_matrix = nmf_para_model.fit_transform(tfidf_para_vectors)\n",
    "H_para_matrix = nmf_para_model.components_\n",
    "\n",
    "# Call the display_topics function separately\n",
    "display_topics(nmf_para_model, tfidf_para_vectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad8eeef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.06189432, 17.0213082 , 13.6565622 , 10.18695452, 11.35821532,\n",
       "        5.95003141,  7.90001101,  4.13937503, 11.91132462,  6.81432338])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_text_matrix.sum(axis=0)/W_text_matrix.sum()*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "113223b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.42905814, 10.3250673 , 10.19143368,  9.97207971,  6.63847604,\n",
       "        7.36011502,  8.92422143,  8.31107274, 16.87412537, 10.97435057])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_para_matrix.sum(axis=0)/W_para_matrix.sum()*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dacb1758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srijanagella/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "\n",
    "# Convert stopwords set to list\n",
    "stopwords_list = list(stopwords)\n",
    "\n",
    "count_para_vectorizer = CountVectorizer(stop_words=stopwords_list, min_df=5, max_df=0.7)\n",
    "count_para_vectors = count_para_vectorizer.fit_transform(paragraph_df[\"text\"])\n",
    "count_para_vectors.shape\n",
    "\n",
    "lda_para_model = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "W_lda_para_matrix = lda_para_model.fit_transform(count_para_vectors)\n",
    "H_lda_para_matrix = lda_para_model.components_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce178421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(279076, 25156)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_para_vectorizer = CountVectorizer(stop_words='english', min_df=5, max_df=0.7)  # or pass your list of stopwords here\n",
    "count_para_vectors = count_para_vectorizer.fit_transform(paragraph_df[\"text\"])\n",
    "count_para_vectors.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29334be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/srijanagella/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Assuming you have a DataFrame named paragraph_df with a column named \"text\" containing the text data\n",
    "paragraph_df = pd.DataFrame({\"text\": [\"Sample text 1\", \"Sample text 2\", \"Sample text 3\"]})\n",
    "\n",
    "# Create CountVectorizer with adjusted stopwords and parameters\n",
    "count_para_vectorizer = CountVectorizer(stop_words=stopwords.words('english'), min_df=1, max_df=1.0)\n",
    "\n",
    "# Fit and transform the text data\n",
    "count_para_vectors = count_para_vectorizer.fit_transform(paragraph_df[\"text\"])\n",
    "count_para_vectors.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6568ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_para_model = LatentDirichletAllocation(n_components = 10,\n",
    "                                           random_state=42)\n",
    "W_lda_para_matrix =lda_para_model.fit_transform(count_para_vectors)\n",
    "H_lda_para_matrix = lda_para_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4c53bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4c0365",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
